\documentclass{article}
\usepackage{amsmath}% amsmath for equations
\usepackage{hyperref}% for links
\usepackage{graphicx}


\begin{document}

\title{\textbf{Decision Tree Learning}}
\author{Assioma Andrea Aligi}
\date{5 febbraio 2024}
\maketitle


{\Large \textbf{1. Introduzione}}\\
Il seguente elaborato mostra la realizzazione di un albero decisione partendo da quello descritto nel capitolo 19.3 del libro Aritficial Intelligence: A Modern Approach  
(di Russell e Norvig (edizione 2020)), al quale viene modificato il passo base dello pseudocodice in modo che la ricorsione viene interrotta se la profondità è maggiore 
di un intero P assegnato oppure se il numero di esempi in examples è minore di un intero M assegnato.\\
\\
Lo pseudocodice preso in considerazione utilizza come metodi di inserimento di nodi all'interno dell'albero i concetti di entropia e di information gain della teoria della teoria
dell'informazione.\\
\\
{\Large \textbf{{\large{1}}.{\small{1}}. Hardware e sistema operativo utilizzati}}\\
Il codice è stato eseguito su un pc fisso con processore i5-10600k e 32gb di ram. Il sistema operativo usato è Arch Linux.\\
\\
{\Large \textbf{2. Creazione dell'albero di decisione}}\\
Per realizzare l'albero di decisione viene estratto il dataset dalla libreria python \texttt{ucimlrepo} tramite id (per esempio l'iris ha come id il 53). Successivamente, viene eseguito uno
shuffle per randomizzare gli elementi all'interno: essendo gli esempi ordinati in base al target, ciò permette di dividere il seguente dataset in training e test 
senza che quest'ultimo abbia istanze di un solo target.\\
Una volta ottenuti i due dataset, viene chiamata la funzione che realizza l'albero: ricorsivamente, viene analizzato ciascun esempio in modo tale da determinare quale attributo
inserire prima come nodo dell'albero, oppure viene aggiornata la frontiera aggiungendo foglie all'albero.\\
Ottenuto l'albero di decisione, viene disegnato graficamente tramite l'utilizzo della libreria \texttt{networkx} e ne viene calcolata la precisione testando sia il training dataset
che il test dataset.\\
\\
I dataset utilizzati sono stati reperiti da \href{https://archive.ics.uci.edu/}{UCI Machine Learning Repository}:
\begin{itemize}
	\item \href{https://archive.ics.uci.edu/dataset/53/iris}{Iris}: classifica le tipologie di iris in base a una serie di attributi. Le variabili sono \texttt{sepal length}, \texttt{sepal width}, \texttt{petal length}, \texttt{sepal length} e \texttt{class} (setosa, versicolor, virginica).
	\item \href{https://archive.ics.uci.edu/dataset/850/raisin}{Raisin}: distingue le tipologie di uvetta, analizzando i pixel di immagini. Le variabili in questo caso sono \texttt{Area} (dell'uvetta in pixel), \texttt{MajorAxisLength} (misura "dell'ambiente" calcolando distanza tra il contorno dell'uvetta e i pixel che lo circondano), \texttt{MinorAxiisLength} (rappresenta la lunghezza del segmento più lungo in pixel che si può creare all'interno dell'uvetta), \texttt{Eccentricity} (rappresenta la lunghezza del segmento più corto che si può creare all'interno dell'uvetta), \texttt{ConvexArea} (determina l'eccentricità dell'ellisse che approssima l'uvetta), \texttt{Extent} (è la dimensione in pixel dell'ellisse che approssima l'uvetta), \texttt{Perimeter} (rappresenta il rapporto tra le dimensioni dell'uvetta e quelle "dell'ambiente"), \texttt{Class} (Kecimen, Besni).
	\item \href{https://archive.ics.uci.edu/dataset/602/dry+bean+dataset}{Dry Bean}: simile al resin, solo che in questo caso vengono analizzati fagioli asciutti. Le variabili sono \texttt{Area} (area del fagiolo), \texttt{Perimeter} (perimetro del bordo del fagiolo), \texttt{MajorAxisLength} (lunghezza del segmento più lungo contenuto dal fagiolo), \texttt{MinorAxisLength} (lunghezza massima del segmento che misura il fagiolo quando è in posizione verticale rispetto a uno degli assi), \texttt{AspetctRatio} (rapporto tra \texttt{MajorAxisLength} e \texttt{MinorAxisLength}), \texttt{Eccentry} (eccentricità dell'ellisse che approssima il fagiolo), \texttt{ConvexArea} (area del poligono (in pixel) che contiene il seme del fagiolo), \texttt{EquivDiameter} (diametro cerchio che ha la stessa area del seme del fagiolo), \texttt{Extent} (rapporto tra i pixel occupati dal fagiolo e quelli occupati dal riquadro dell'immagine), \texttt{Solidity} ($\frac{4\pi A}{P^2}$), \texttt{Compactness} (misura la rotondità di un oggetto), \texttt{ShapeFactor1}, \texttt{ShapeFactor2}, \texttt{Shapefactor3}, \texttt{Shapefactor4}, \texttt{Class} (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz e Sira)
\end{itemize}
Per la realizzazione dell'albero, sono stati realizzati 5 files differenti:
\begin{itemize}
	\item \texttt{main.py}: si seleziona l'id del dataset, lo split train/test, la massima profondità dell'albero (\texttt{P}) e il numero minimo di examples (\texttt{M}). Successivamente, chiama la funzione che crea l'albero, lo disegna e stampa la precisione (\texttt{create\_tree}).
	\item \texttt{tree\_creation.py}: definisce la funzione \texttt{create\_tree} in cui viene inizializzati i dataset e creato l'albero di decisione tramite l'istanziazione di una variabile \texttt{DecisionTree}. Vengono inoltre calcolate e stampate le precisioni tramite la chiamata alla funzione \texttt{test\_tree}, e viene disegnato l'albero tramite la chiamata alla funzione \texttt{plot\_tree} e rappresenato graficamente grazie alla libreria \texttt{matplotlib}.
	\item \texttt{decision\_tree.py}: definisce la classe \texttt{DecisionTree}.
	\item \texttt{tree\_elements.py}: definisce le classi \texttt{Branch} e \texttt{Node} necessarie per la realizzazione dell'albero di decisione.
	\item \texttt{plot\_tree.py}: definisce la funzione che disegna l'albero di decisione.
	\item \texttt{tree\_testing.py}: definisce le funzioni che permetto il calcolo della precisione dell'albero (\texttt{test\_tree} e \texttt{tree\_precision}).
\end{itemize}

\end{document}
