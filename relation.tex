\documentclass{article}
\usepackage{amsmath}% amsmath for equations
\usepackage{hyperref}% for links
\hypersetup{
	colorlinks = true,
	urlcolor = blue,
}
\usepackage{graphicx}


\begin{document}

\title{\textbf{Decision Tree Learning}}
\author{Assioma Andrea Aligi}
\date{5 febbraio 2024}
\maketitle


{\Large \textbf{1. Introduzione}}\\
Il seguente elaborato mostra la realizzazione di un albero decisione partendo da quello descritto nel capitolo 19.3 del libro Aritficial Intelligence: A Modern Approach  
(di Russell e Norvig (edizione 2020)), al quale viene modificato il passo base dello pseudocodice in modo che la ricorsione viene interrotta se la profondità è maggiore 
di un intero P assegnato oppure se il numero di esempi in examples è minore di un intero M assegnato.\\
\\
Lo pseudocodice preso in considerazione utilizza come metodi di inserimento di nodi all'interno dell'albero i concetti di entropia e di information gain della teoria della teoria
dell'informazione.\\
\\
{\Large \textbf{{\large{1}}.{\small{1}}. Hardware e sistema operativo utilizzati}}\\
Il codice è stato eseguito su un pc fisso con processore i5-10600k e 32gb di ram. Il sistema operativo usato è Arch Linux.\\
\\
{\Large \textbf{2. Creazione dell'albero di decisione}}\\
Per realizzare l'albero di decisione viene estratto il dataset dalla libreria python \texttt{ucimlrepo} tramite id (per esempio l'iris ha come id il 53). Successivamente, viene eseguito uno
shuffle per randomizzare gli elementi all'interno: essendo gli esempi ordinati in base al target, ciò permette di dividere il seguente dataset in training e test 
senza che quest'ultimo abbia istanze di un solo target.\\
Una volta ottenuti i due dataset, viene chiamata la funzione che realizza l'albero: ricorsivamente, viene analizzato ciascun esempio in modo tale da determinare quale attributo
inserire prima come nodo dell'albero, oppure viene aggiornata la frontiera aggiungendo foglie all'albero.\\
Ottenuto l'albero di decisione, viene disegnato graficamente tramite l'utilizzo della libreria \texttt{networkx} e ne viene calcolata la precisione testando sia il training dataset
che il test dataset.\\
\\
I dataset utilizzati sono stati reperiti da \href{https://archive.ics.uci.edu/}{UCI Machine Learning Repository}:
\begin{itemize}
	\item \href{https://archive.ics.uci.edu/dataset/53/iris}{Iris}: classifica le tipologie di iris in base a una serie di attributi. Le variabili sono \texttt{sepal length}, \texttt{sepal width}, \texttt{petal length}, \texttt{sepal length} e \texttt{class} (setosa, versicolor, virginica).
	\item \href{https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records}{Heart failure}: \texttt{age}, \texttt{anemia}, \texttt{creatinine\_phosphofinase} (indica il livello dell'enzima CPK nel sangue), \texttt{diabetes}, \texttt{ejection\_fraction} (percentuale di sangue che esce dal cuore a ogni contrazione), \texttt{high\_blood\_pressure}, \texttt{platelets}, \texttt{serum\_creatinine} (livello di siero di creatinine nel sangue), \texttt{serum\_sodium} (livello di siero di sodio nel sangue), \texttt{sex}, \texttt{smoking}, \texttt{time} (periodo di follow-up), \texttt{death\_event} (se il paziente muore durante il periodo follow-up (target)).
	\item \href{https://archive.ics.uci.edu/dataset/109/wine}{Wine}: \texttt{Alcohol}, \texttt{Maliacid}, \texttt{Ash}, \texttt{Alcalinity\_of\_ash}, \texttt{Magnesium}, \texttt{Total\_phenols}, \texttt{Flavanoids}, \texttt{Nonflavanoid\_phenols}, \texttt{Proathocyanins}, \texttt{Color\_intensity}, \texttt{Hue}, \texttt{0D280\_0D315\_of\_diluted\_wines}, \texttt{Proline}, \texttt{class} (tipo di vino).
\end{itemize}
Per la realizzazione dell'albero, sono stati realizzati 5 files differenti:
\begin{itemize}
	\item \texttt{main.py}: si seleziona l'id del dataset, lo split train/test, la massima profondità dell'albero (\texttt{P}) e il numero minimo di examples (\texttt{M}). Successivamente, chiama la funzione che crea l'albero, lo disegna e stampa la precisione (\texttt{create\_tree}).
	\item \texttt{tree\_creation.py}: definisce la funzione \texttt{create\_tree} in cui viene inizializzati i dataset e creato l'albero di decisione tramite l'istanziazione di una variabile \texttt{DecisionTree}. Vengono inoltre calcolate e stampate le precisioni tramite la chiamata alla funzione \texttt{test\_tree}, e viene disegnato l'albero tramite la chiamata alla funzione \texttt{plot\_tree} e rappresenato graficamente grazie alla libreria \texttt{matplotlib}.
	\item \texttt{decision\_tree.py}: definisce la classe \texttt{DecisionTree}.
	\item \texttt{tree\_elements.py}: definisce le classi \texttt{Branch} e \texttt{Node} necessarie per la realizzazione dell'albero di decisione.
	\item \texttt{plot\_tree.py}: definisce la funzione che disegna l'albero di decisione.
	\item \texttt{tree\_testing.py}: definisce le funzioni che permetto il calcolo della precisione dell'albero (\texttt{test\_tree} e \texttt{tree\_precision}).
\end{itemize}
{\Large \textbf{3. Analisi dei risultati}}\\
I risultati dei vari dataset sono stati i seguenti\\
\\
{\Large \textbf{{\large{3}}.{\small{1}}. Dataset Iris}}\\


\end{document}
